{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1386555,"sourceType":"datasetVersion","datasetId":809358},{"sourceId":11118714,"sourceType":"datasetVersion","datasetId":6933177},{"sourceId":11119237,"sourceType":"datasetVersion","datasetId":6933604},{"sourceId":11130939,"sourceType":"datasetVersion","datasetId":6942121}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Face Mask Detection using Deep Learning**\n\n## **Introduction**\nFace mask detection is a crucial application of deep learning and computer vision, especially in public health and safety. In this notebook, we implement a model to detect whether a person is wearing a mask or not using a Convolutional Neural Network (CNN) trained on labeled images.\n\n## **Objectives**\n- Build a deep learning model to classify masked and unmasked faces.\n- Utilize OpenCVâ€™s deep learning module for face detection.\n- Apply the trained model to static images and pre-recorded videos.\n- Save the trained model for future use.\n\n## **Dataset**\nWe use a dataset containing images of individuals:\n- **With Mask** ðŸŸ©\n- **Without Mask** ðŸŸ¥  \n\nThe dataset is preprocessed to ensure balanced representation and optimal model performance.\n\n## **Workflow**\n1. **Data Preprocessing**  \n   - Load and augment image data.  \n   - Convert images to numerical arrays and normalize pixel values.  \n   - Split into training and validation sets.  \n\n2. **Model Training**  \n   - Train a CNN-based model or leverage a pre-trained network (e.g., MobileNetV2).  \n   - Use appropriate loss functions and optimizers.  \n\n3. **Evaluation & Model Saving**  \n   - Assess performance using accuracy and loss metrics.  \n   - Save the trained model to avoid re-training in future runs.  \n\n4. **Face Mask Detection in Images & Videos**  \n   - Use OpenCVâ€™s DNN face detector to locate faces.  \n   - Apply the trained model to classify mask usage.  \n   - Display results with bounding boxes and labels.  \n\n## **Implementation**\nWe implement the project using:\n- **TensorFlow/Keras**: For model training and inference.\n- **OpenCV**: For face detection.\n- **Matplotlib**: For visualizing results.\n\n## **Model Performance**\n- **test_loss**:  0.054265428334474564\n- **test_accuracy**:  0.979345977306366\n- **Memory consumption** : 0 bytes\n## **Using the Saved Model**\nOnce trained, the model is saved as a `.keras` file. To reuse it:\n```python\nfrom tensorflow.keras.models import load_model\nmodel = load_model(\"mask_detector.keras\")\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2, Xception, VGG16, InceptionV3\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Dropout, \\\n                                    Flatten, Dense, BatchNormalization, \\\n                                    SpatialDropout2D, AveragePooling2D, Input\n\nimport os\nimport cv2 \nimport warnings\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ntf.get_logger().setLevel('WARNING')\n#removed cnn and xception \nparser = argparse.ArgumentParser()\nparser.add_argument('-d', '--data-dir', type=str, default='/kaggle/input/face-mask-dataset/data',\n                    help=\"Directory of dataset\")\nparser.add_argument('-e', '--epochs', type=int, default=30,\n                    help=\"Where to write the new data\")\nparser.add_argument(\"-m\", \"--model\", type=str, default=\"mask_detector.keras\",\n                    help=\"Path to output face mask detector model\")\nparser.add_argument('-s', '--size', type=int, default=64,\n                    help=\"Size of input data\")\nparser.add_argument('-b', '--batch-size', type=int, default=32,\n                    help=\"Bactch size of data generator\")\nparser.add_argument('-l', '--learning-rate', type=float, default=0.0001,\n                    help=\"Learning rate value\")\nparser.add_argument('-sh', '--show-history', action='store_true',\n                    help=\"Show training history\")\nparser.add_argument('-n', '--net-type', type=str, default='MobileNetV2',\n                    choices=[ 'CNN','MobileNetV2', 'VGG16','Xception'],\n                    help=\"The network architecture, optional: MobileNetV2, VGG16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.197015Z","iopub.execute_input":"2025-03-22T05:21:29.197316Z","iopub.status.idle":"2025-03-22T05:21:29.212590Z","shell.execute_reply.started":"2025-03-22T05:21:29.197291Z","shell.execute_reply":"2025-03-22T05:21:29.211871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef CNN_model(learning_rate, input_shape):\n    # Build model\n    model = Sequential()\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=input_shape, activation='relu'))\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=input_shape, activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n\n    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], \\\n                  optimizer=Adam(learning_rate=learning_rate))\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.213892Z","iopub.execute_input":"2025-03-22T05:21:29.214171Z","iopub.status.idle":"2025-03-22T05:21:29.226623Z","shell.execute_reply.started":"2025-03-22T05:21:29.214152Z","shell.execute_reply":"2025-03-22T05:21:29.225960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef MobileNetV2_model(learning_rate, input_shape):\n    baseModel = MobileNetV2(include_top=False, input_tensor=Input(shape=input_shape))\n    for layer in baseModel.layers[:-4]:\n        layer.trainable = False\n\n    model = Sequential()\n    model.add(baseModel)\n    model.add(AveragePooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(512, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile our model\n    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], \\\n                  optimizer=Adam(learning_rate=learning_rate))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.227690Z","iopub.execute_input":"2025-03-22T05:21:29.227901Z","iopub.status.idle":"2025-03-22T05:21:29.250151Z","shell.execute_reply.started":"2025-03-22T05:21:29.227882Z","shell.execute_reply":"2025-03-22T05:21:29.249456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef VGG16_model(learning_rate, input_shape):\n    baseModel = VGG16(include_top=False, input_tensor=Input(shape=input_shape))\n    for layer in baseModel.layers:\n        layer.trainable = False\n\n    model = Sequential()\n    model.add(baseModel)\n    model.add(AveragePooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(512, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile our model\n    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], \\\n                  optimizer=Adam(learning_rate=learning_rate))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.250780Z","iopub.execute_input":"2025-03-22T05:21:29.251010Z","iopub.status.idle":"2025-03-22T05:21:29.264227Z","shell.execute_reply.started":"2025-03-22T05:21:29.250980Z","shell.execute_reply":"2025-03-22T05:21:29.263667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef Xception_model(learning_rate, input_shape):\n    baseModel = Xception(include_top=False, input_tensor=Input(shape=input_shape))\n    for layer in baseModel.layers:\n        layer.trainable = False\n\n    model = Sequential()\n    model.add(baseModel)\n    model.add(AveragePooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(512, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(50, activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile our model\n    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], \\\n                  optimizer=Adam(learning_rate=learning_rate))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.264937Z","iopub.execute_input":"2025-03-22T05:21:29.265197Z","iopub.status.idle":"2025-03-22T05:21:29.282478Z","shell.execute_reply.started":"2025-03-22T05:21:29.265171Z","shell.execute_reply":"2025-03-22T05:21:29.281718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def keras_model_memory_usage_in_bytes(model, batch_size):\n    default_dtype = tf.keras.backend.floatx()\n    total_memory = 0\n    for layer in model.layers:\n        # Skip layers that do not have an output_shape attribute\n        if not hasattr(layer, 'output_shape'):\n            continue\n\n        # If the layer is a nested model, calculate its memory recursively.\n        if isinstance(layer, tf.keras.Model):\n            total_memory += keras_model_memory_usage_in_bytes(layer, batch_size=batch_size)\n            continue\n\n        # Get the size (in bytes) of one element of the layer's dtype.\n        single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n\n        out_shape = layer.output_shape\n        if isinstance(out_shape, list):\n            # Use the first output shape if the layer has multiple outputs.\n            out_shape = out_shape[0]\n        \n        # Calculate total number of elements in the output shape,\n        # replacing None dimensions (typically the batch size) with the actual batch_size.\n        total_elements = 1\n        for dim in out_shape:\n            if dim is None:\n                total_elements *= batch_size\n            else:\n                total_elements *= dim\n\n        total_memory += single_layer_mem * total_elements\n\n    return total_memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.283270Z","iopub.execute_input":"2025-03-22T05:21:29.283560Z","iopub.status.idle":"2025-03-22T05:21:29.301673Z","shell.execute_reply.started":"2025-03-22T05:21:29.283541Z","shell.execute_reply":"2025-03-22T05:21:29.300957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n\n    args, unknown = parser.parse_known_args()\n\n    bs = args.batch_size\n    lr = args.learning_rate\n    size = (args.size, args.size)\n    shape = (args.size, args.size, 3)\n    epochs = args.epochs\n\n    # Load and preprocess data\n    train_dir = os.path.join(args.data_dir)\n    test_dir = os.path.join(args.data_dir)\n    valid_dir = os.path.join(args.data_dir)\n\n    train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=5, zoom_range=0.2, \\\n                                       shear_range=0.2, brightness_range=[0.9, 1.1], \\\n                                       horizontal_flip=True)                  \n    valid_datagen = ImageDataGenerator(rescale=1./255, rotation_range=5, zoom_range=0.2, \\\n                                       shear_range=0.2, brightness_range=[0.9, 1.1], \\\n                                       horizontal_flip=True)\n    test_datagen  = ImageDataGenerator(rescale=1./255)\n\n    train_generator = train_datagen.flow_from_directory(train_dir, target_size=size, shuffle=True,\n                                                        batch_size=bs, class_mode='binary')\n    valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=size, shuffle=True,\n                                                        batch_size=bs, class_mode='binary')\n    test_generator  = test_datagen.flow_from_directory(test_dir, target_size=size, shuffle=True,\n                                                        batch_size=bs, class_mode='binary')\n\n    print(train_generator.class_indices)\n    print(train_generator.image_shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:29.303637Z","iopub.execute_input":"2025-03-22T05:21:29.303838Z","iopub.status.idle":"2025-03-22T05:21:31.924477Z","shell.execute_reply.started":"2025-03-22T05:21:29.303821Z","shell.execute_reply":"2025-03-22T05:21:31.923599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # Build model\nnet_type_to_model = {\n        'CNN' : CNN_model,\n        'MobileNetV2': MobileNetV2_model,\n        'VGG16' : VGG16_model,\n        'Xception' : Xception_model\n    }\nmodel_name = args.net_type\nmodel_builder = net_type_to_model.get(model_name)\nmodel = model_builder(lr, shape)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:31.925829Z","iopub.execute_input":"2025-03-22T05:21:31.926164Z","iopub.status.idle":"2025-03-22T05:21:35.433685Z","shell.execute_reply.started":"2025-03-22T05:21:31.926128Z","shell.execute_reply":"2025-03-22T05:21:35.432845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nearlystop = EarlyStopping(monitor='val_loss', patience=5, mode='auto')\n\ntensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n\ncheckpoint = ModelCheckpoint(os.path.join(\"results\", f\"{model_name}\" + f\"-size-{size[0]}\" + \\\n                                             f\"-bs-{bs}\" + f\"-lr-{lr}.keras\"),\n                             monitor='val_loss', save_best_only=True, verbose=1)\n# Train model\nhistory = model.fit(train_generator, epochs=epochs, validation_data=valid_generator,\n                    batch_size=bs, callbacks=[earlystop, tensorboard, checkpoint], shuffle=True)\ntest_loss, test_accuracy = model.evaluate(test_generator)   \nmetrics = pd.DataFrame(history.history)\nprint(metrics.head(10))\n\nprint('test_loss: ', test_loss)\nprint('test_accuracy: ', test_accuracy)\nprint('Memory consumption: %s bytes' % keras_model_memory_usage_in_bytes(model, batch_size=bs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:21:35.434543Z","iopub.execute_input":"2025-03-22T05:21:35.434850Z","iopub.status.idle":"2025-03-22T05:48:19.806566Z","shell.execute_reply.started":"2025-03-22T05:21:35.434813Z","shell.execute_reply":"2025-03-22T05:48:19.805849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_generator)   \nmetrics = pd.DataFrame(history.history)\nprint(metrics.head(10))\n\nprint('test_loss: ', test_loss)\nprint('test_accuracy: ', test_accuracy)\nprint('Memory consumption: %s bytes' % keras_model_memory_usage_in_bytes(model, batch_size=bs))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:48:19.807456Z","iopub.execute_input":"2025-03-22T05:48:19.808002Z","iopub.status.idle":"2025-03-22T05:48:35.346983Z","shell.execute_reply.started":"2025-03-22T05:48:19.807978Z","shell.execute_reply":"2025-03-22T05:48:35.346232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(args.model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:48:35.347838Z","iopub.execute_input":"2025-03-22T05:48:35.348122Z","iopub.status.idle":"2025-03-22T05:48:35.352174Z","shell.execute_reply.started":"2025-03-22T05:48:35.348099Z","shell.execute_reply":"2025-03-22T05:48:35.351464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# serialize the model to disk\nprint(\"saving mask detector model...\")\n#must add .keras or .h5 when you input the the save file path\nargs, unknown = parser.parse_known_args()\nargs.model = \"mask_detector.keras\"  # Manually override to ensure the correct extension.\nmodel.save(args.model)\n\n\nif args.show_history:\n        plt.subplot(211)\n        plt.title('Loss')\n        plt.plot(history.history['loss'], label='train')\n        plt.plot(history.history['val_loss'], label='test')\n        plt.legend()\n\n        plt.subplot(212)\n        plt.title('Accuracy')\n        plt.plot(history.history['accuracy'], label='train')\n        plt.plot(history.history['val_accuracy'], label='test')\n        plt.legend()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:48:35.353002Z","iopub.execute_input":"2025-03-22T05:48:35.353210Z","iopub.status.idle":"2025-03-22T05:48:35.723718Z","shell.execute_reply.started":"2025-03-22T05:48:35.353190Z","shell.execute_reply":"2025-03-22T05:48:35.722672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n\nplt.subplot(212)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:48:35.724690Z","iopub.execute_input":"2025-03-22T05:48:35.724990Z","iopub.status.idle":"2025-03-22T05:48:36.055337Z","shell.execute_reply.started":"2025-03-22T05:48:35.724961Z","shell.execute_reply":"2025-03-22T05:48:36.054359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" !mkdir -p face_detector\n !wget -O face_detector/deploy.prototxt https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n !wget -O face_detector/res10_300x300_ssd_iter_140000.caffemodel https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\nparser.add_argument(\"-fd\", \"--faced\", type=str, default=\"/kaggle/input/face-detector\",\n                    help=\"Path to face detector model directory\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T05:48:36.056220Z","iopub.execute_input":"2025-03-22T05:48:36.056548Z","iopub.status.idle":"2025-03-22T05:48:37.634456Z","shell.execute_reply.started":"2025-03-22T05:48:36.056517Z","shell.execute_reply":"2025-03-22T05:48:37.633623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications.mobilenet_v2 import preprocess_input  # type: ignore\nfrom tensorflow.keras.preprocessing.image import img_to_array  # type: ignore\nfrom tensorflow.keras.models import load_model  # type: ignore\nimport numpy as np\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\n\n# Set paths and parameters\nimage_path = \"/kaggle/input/multiple-mask-nomask-test/masks-tokyo-gty-ps-230314_1678802239571_hpMain.jpg\"\nface_detector_path = \"/kaggle/working/face_detector\"  \nmodel_path = \"/kaggle/working/mask_detector.keras\"       \nsize = 64\nconfidence_threshold = 0.55  # You may lower this if needed\n\ndef mask_image(image_path, face_detector_path, model_path, size, confidence_threshold):\n    # Load the face detector model files\n    prototxtPath = os.path.sep.join([face_detector_path, \"deploy.prototxt\"])\n    weightsPath = os.path.sep.join([face_detector_path, \"res10_300x300_ssd_iter_140000.caffemodel\"])\n    print(\"Loading face detector from:\", prototxtPath, weightsPath)\n    \n    try:\n        net = cv2.dnn.readNetFromCaffe(prototxtPath, weightsPath)\n    except Exception as e:\n        print(\"Error loading face detector:\", e)\n        return\n\n    # Load the mask detector model\n    print(\"Loading mask detector model from:\", model_path)\n    try:\n        model = load_model(model_path)\n    except Exception as e:\n        print(\"Error loading mask detector model:\", e)\n        return\n\n    # Read the input image\n    image = cv2.imread(image_path)\n    if image is None:\n        print(\"Cannot read file:\", image_path)\n        return\n    print(\"Image shape:\", image.shape)\n\n    # If the image is smaller than 300x300, resize it so the face detector works correctly\n    if image.shape[0] < 300 or image.shape[1] < 300:\n        print(\"Image too small, resizing to 300x300\")\n        image = cv2.resize(image, (300, 300))\n\n    # Create blob with fixed size of 300x300 (expected by the face detector)\n    blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(300, 300), mean=(104.0, 177.0, 123.0))\n    print(\"Blob shape:\", blob.shape)\n\n    net.setInput(blob)\n    detections = net.forward()\n    num_detections = detections.shape[2]\n    print(\"Number of detections:\", num_detections)\n\n    found_face = False\n\n    for i in range(0, num_detections):\n        conf = detections[0, 0, i, 2]\n        if conf < confidence_threshold:\n            continue\n\n        box = detections[0, 0, i, 3:7] * np.array([image.shape[1], image.shape[0],\n                                                     image.shape[1], image.shape[0]])\n        (startX, startY, endX, endY) = box.astype(\"int\")\n        startX, startY = max(0, startX), max(0, startY)\n        endX, endY = min(image.shape[1] - 1, endX), min(image.shape[0] - 1, endY)\n\n        # Extract the face region and check if it's valid\n        face_region = image[startY:endY, startX:endX]\n        if face_region.size == 0:\n            print(\"Empty face region detected; skipping detection\", i)\n            continue\n\n        try:\n            found_face = True\n            face = cv2.cvtColor(face_region, cv2.COLOR_BGR2RGB)\n            face = cv2.resize(face, (size, size))\n            face = img_to_array(face)\n            face = preprocess_input(face)\n            face = np.expand_dims(face, axis=0)\n\n            prediction = model.predict(face)[0]\n            label = \"Mask\" if prediction < 0.5 else \"No Mask\"\n            color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n\n            cv2.putText(image, label, (startX, startY - 10), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)\n            cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n            print(f\"Detection {i}: {label} with confidence {conf:.2f}\")\n        except Exception as e:\n            print(f\"Error processing detection {i}: {e}\")\n\n    if not found_face:\n        print(\"No faces detected above the confidence threshold.\")\n    \n    # Convert image from BGR to RGB and display using matplotlib\n    output_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.figure(figsize=(10, 8))\n    plt.imshow(output_image)\n    plt.title(\"Mask Detection Output\")\n    plt.axis(\"off\")\n    plt.show()\n\n# Call the function\nmask_image(image_path, face_detector_path, model_path, size, confidence_threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:13:14.407298Z","iopub.execute_input":"2025-03-22T06:13:14.407627Z","iopub.status.idle":"2025-03-22T06:13:18.983215Z","shell.execute_reply.started":"2025-03-22T06:13:14.407603Z","shell.execute_reply":"2025-03-22T06:13:18.982314Z"}},"outputs":[],"execution_count":null}]}